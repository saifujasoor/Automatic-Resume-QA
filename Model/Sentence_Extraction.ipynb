{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 16:48:53.423017: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-04 16:48:53.423059: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(PATH):\n",
    "    '''For each paras.txt extract sentences for each paragraph using bs4.'''\n",
    "    \n",
    "    sent_file = open(os.path.join(PATH,'sentences.txt'),'w')\n",
    "    \n",
    "    with open(os.path.join(PATH,'paras.txt')) as fobj:\n",
    "        #read the file line-by-line\n",
    "        for line in fobj:\n",
    "            if line != '\\n' and line.strip().startswith('<p>'):\n",
    "                try:\n",
    "                    #extract tagless paragraph-text from <p> tags.\n",
    "                    soup = BeautifulSoup(line.strip(),\"lxml\")\n",
    "                    \n",
    "                    #divide this tagless paragraphs into proper sentences using NLP via spacy.\n",
    "                    doc = nlp(soup.p.text)\n",
    "                except:\n",
    "                    #If the line can't be parsed then log the line and continue to next line.\n",
    "                    logging.warning(PATH,\":\",line,\"can't be parsed.\")\n",
    "                    continue\n",
    "                    \n",
    "                #If parsed then write each sentence in the file 'sentences.txt'.\n",
    "                for each in doc.sents:\n",
    "                    text = each.text+'\\n'\n",
    "                    sent_file.write(each.text+'\\n')\n",
    "                    \n",
    "    sent_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-04 16:52:14,222 : INFO : Generating sentences for dataset1\n",
      "2022-05-04 16:54:58,587 : INFO : SUCCESS\n",
      "2022-05-04 16:54:58,587 : INFO : Generating sentences for dataset35\n",
      "2022-05-04 16:55:15,346 : INFO : SUCCESS\n",
      "2022-05-04 16:55:15,347 : INFO : Generating sentences for dataset41\n",
      "2022-05-04 16:55:45,276 : INFO : SUCCESS\n",
      "2022-05-04 16:55:45,277 : INFO : Generating sentences for dataset5\n",
      "2022-05-04 17:06:05,599 : INFO : SUCCESS\n",
      "2022-05-04 17:06:05,600 : INFO : Generating sentences for dataset11\n",
      "2022-05-04 17:06:14,778 : INFO : SUCCESS\n",
      "2022-05-04 17:06:14,779 : INFO : Generating sentences for dataset23\n",
      "2022-05-04 17:07:07,845 : INFO : SUCCESS\n",
      "2022-05-04 17:07:07,846 : INFO : Generating sentences for dataset12\n",
      "2022-05-04 17:07:15,849 : INFO : SUCCESS\n",
      "2022-05-04 17:07:15,850 : INFO : Generating sentences for dataset34\n",
      "2022-05-04 17:07:34,542 : INFO : SUCCESS\n",
      "2022-05-04 17:07:34,542 : INFO : Generating sentences for dataset32\n",
      "2022-05-04 17:09:15,352 : INFO : SUCCESS\n",
      "2022-05-04 17:09:15,353 : INFO : Generating sentences for dataset14\n",
      "2022-05-04 17:10:03,550 : INFO : SUCCESS\n",
      "2022-05-04 17:10:03,550 : INFO : Generating sentences for dataset7\n",
      "2022-05-04 17:10:05,187 : INFO : SUCCESS\n",
      "2022-05-04 17:10:05,188 : INFO : Generating sentences for dataset43\n",
      "2022-05-04 17:11:09,237 : INFO : SUCCESS\n",
      "2022-05-04 17:11:09,237 : INFO : Generating sentences for dataset31\n",
      "2022-05-04 18:03:39,549 : INFO : SUCCESS\n",
      "2022-05-04 18:03:39,550 : INFO : Generating sentences for dataset21\n",
      "2022-05-04 18:05:07,546 : INFO : SUCCESS\n",
      "2022-05-04 18:05:07,547 : INFO : Generating sentences for dataset25\n",
      "2022-05-04 18:05:36,539 : INFO : SUCCESS\n",
      "2022-05-04 18:05:36,539 : INFO : Generating sentences for dataset24\n",
      "2022-05-04 18:05:47,559 : INFO : SUCCESS\n",
      "2022-05-04 18:05:47,560 : INFO : Generating sentences for dataset37\n",
      "2022-05-04 18:06:50,966 : INFO : SUCCESS\n",
      "2022-05-04 18:06:50,967 : INFO : Generating sentences for dataset8\n",
      "2022-05-04 18:07:29,491 : INFO : SUCCESS\n",
      "2022-05-04 18:07:29,492 : INFO : Generating sentences for dataset26\n",
      "2022-05-04 18:09:25,024 : INFO : SUCCESS\n",
      "2022-05-04 18:09:25,024 : INFO : Generating sentences for dataset4\n",
      "2022-05-04 18:09:31,027 : INFO : SUCCESS\n",
      "2022-05-04 18:09:31,028 : INFO : Generating sentences for dataset38\n",
      "2022-05-04 18:09:41,092 : INFO : SUCCESS\n",
      "2022-05-04 18:09:41,093 : INFO : Generating sentences for dataset42\n",
      "2022-05-04 18:10:36,390 : INFO : SUCCESS\n",
      "2022-05-04 18:10:36,390 : INFO : Generating sentences for dataset30\n",
      "2022-05-04 18:10:37,386 : INFO : SUCCESS\n",
      "2022-05-04 18:10:37,387 : INFO : Generating sentences for dataset22\n",
      "2022-05-04 18:11:23,462 : INFO : SUCCESS\n",
      "2022-05-04 18:11:23,462 : INFO : Generating sentences for dataset3\n",
      "2022-05-04 18:11:27,305 : INFO : SUCCESS\n",
      "2022-05-04 18:11:27,305 : INFO : Generating sentences for dataset28\n",
      "2022-05-04 18:12:19,245 : INFO : SUCCESS\n",
      "2022-05-04 18:12:19,246 : INFO : Generating sentences for dataset13\n",
      "2022-05-04 18:13:23,965 : INFO : SUCCESS\n",
      "2022-05-04 18:13:23,966 : INFO : Generating sentences for dataset36\n",
      "2022-05-04 18:16:24,074 : INFO : SUCCESS\n",
      "2022-05-04 18:16:24,075 : INFO : Generating sentences for dataset19\n",
      "2022-05-04 18:18:48,166 : INFO : SUCCESS\n",
      "2022-05-04 18:18:48,167 : INFO : Generating sentences for dataset15\n",
      "2022-05-04 18:19:12,737 : INFO : SUCCESS\n",
      "2022-05-04 18:19:12,738 : INFO : Generating sentences for dataset17\n",
      "2022-05-04 18:19:20,842 : INFO : SUCCESS\n",
      "2022-05-04 18:19:20,843 : INFO : Generating sentences for dataset16\n",
      "2022-05-04 18:20:00,112 : INFO : SUCCESS\n",
      "2022-05-04 18:20:00,113 : INFO : Generating sentences for dataset20\n",
      "2022-05-04 18:20:30,503 : INFO : SUCCESS\n",
      "2022-05-04 18:20:30,504 : INFO : Generating sentences for dataset40\n",
      "2022-05-04 18:20:53,846 : INFO : SUCCESS\n",
      "2022-05-04 18:20:53,846 : INFO : Generating sentences for dataset29\n",
      "2022-05-04 18:21:05,003 : INFO : SUCCESS\n",
      "2022-05-04 18:21:05,004 : INFO : Generating sentences for dataset9\n",
      "2022-05-04 18:21:16,535 : INFO : SUCCESS\n",
      "2022-05-04 18:21:16,535 : INFO : Generating sentences for dataset39\n",
      "2022-05-04 18:21:28,729 : INFO : SUCCESS\n",
      "2022-05-04 18:21:28,730 : INFO : Generating sentences for dataset27\n",
      "2022-05-04 18:22:51,421 : INFO : SUCCESS\n",
      "2022-05-04 18:22:51,422 : INFO : Generating sentences for dataset2\n",
      "2022-05-04 18:22:59,945 : INFO : SUCCESS\n",
      "2022-05-04 18:22:59,946 : INFO : Generating sentences for dataset10\n",
      "2022-05-04 18:24:09,639 : INFO : SUCCESS\n",
      "2022-05-04 18:24:09,640 : INFO : Generating sentences for dataset18\n",
      "2022-05-04 18:25:43,683 : INFO : SUCCESS\n",
      "2022-05-04 18:25:43,683 : INFO : Generating sentences for dataset33\n",
      "2022-05-04 18:25:52,355 : INFO : SUCCESS\n"
     ]
    }
   ],
   "source": [
    "dirs = os.listdir('/home/saifullah/Desktop/Automatic-Resume-QA/Model/stackexchange')\n",
    "#print len(dirs)\n",
    "for each in dirs:\n",
    "    if each == '.DS_Store':\n",
    "        continue\n",
    "    PATH = os.path.join('/home/saifullah/Desktop/Automatic-Resume-QA/Model/stackexchange',each)\n",
    "    logging.info('Generating sentences for ' + each)\n",
    "    get_sentences(PATH)\n",
    "    logging.info('SUCCESS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
